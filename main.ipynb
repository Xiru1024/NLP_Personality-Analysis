{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a65c07e0",
   "metadata": {},
   "source": [
    "### 1.\n",
    " We want to explore the general trend of the conversation data. For this purpose, create a dataframe for each speaker by gathering all his utterances in a single file.\n",
    "Then write a script that determines the vocabulary set, vocabulary size, total number of tokens, total number of repetitions of words in the same post, total number of confirmation words (e.g., yes, OK, sure), total number of negation tokens, associated to each speaker. Summarize the result in a table, and then draw a subgraph that shows on the same plot the evolution of number of repetitions, number of negation, number of confirmation-like tokens, with respect to the number of tokens employed for each speaker (You may create some subdivision from the total number of tokens to ensure enough datum are used to represent the graphical illustration). Calculate the overall personality for each speaker by averaging over all instances of the original dataset, and comment on possible similarities and differences between speakers and whether some attributes are more associated with some personality patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8678604f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset from GitHub repository\n",
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/preke/PELD/main/data/Dyadic_PELD.tsv\"\n",
    "df = pd.read_csv(url, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e87aa7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6510 entries, 0 to 6509\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Speaker_1    6510 non-null   object\n",
      " 1   Speaker_2    6510 non-null   object\n",
      " 2   Personality  6510 non-null   object\n",
      " 3   Utterance_1  6510 non-null   object\n",
      " 4   Utterance_2  6510 non-null   object\n",
      " 5   Utterance_3  6510 non-null   object\n",
      " 6   Emotion_1    6510 non-null   object\n",
      " 7   Emotion_2    6510 non-null   object\n",
      " 8   Emotion_3    6510 non-null   object\n",
      " 9   Sentiment_1  6510 non-null   object\n",
      " 10  Sentiment_2  6510 non-null   object\n",
      " 11  Sentiment_3  6510 non-null   object\n",
      "dtypes: object(12)\n",
      "memory usage: 610.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "26c81685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speaker 1:  ['Chandler' 'Joey' 'Rachel' 'Monica' 'Phoebe' 'Ross']\n"
     ]
    }
   ],
   "source": [
    "# save each speaker with their utterances in separate files\n",
    "distinct_speakers1 = df['Speaker_1'].unique()\n",
    "print('speaker 1: ', distinct_speakers1)\n",
    "for s in distinct_speakers1:\n",
    "    lines = (pd.concat([df.loc[df['Speaker_1']==s, 'Utterance_1'],\n",
    "                        df.loc[df['Speaker_1']==s, 'Utterance_3'],\n",
    "                        df.loc[df['Speaker_2']==s, 'Utterance_2']])\n",
    "               .dropna()\n",
    "               .rename('utterance'))\n",
    "    lines.to_csv(f'{s}_utterances.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "27da9b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load each speaker's utterances with a dataframe from the saved files\n",
    "df_Chandler = pd.read_csv('Chandler_utterances.csv')\n",
    "df_Monica = pd.read_csv('Monica_utterances.csv')\n",
    "df_Ross = pd.read_csv('Ross_utterances.csv')\n",
    "df_Rachel = pd.read_csv('Rachel_utterances.csv')\n",
    "df_Joey = pd.read_csv('Joey_utterances.csv')\n",
    "df_Phoebe = pd.read_csv('Phoebe_utterances.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4ffbe046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O.K.', 'OK', 'Oklahoma', 'Sooner State', 'absolutely', 'adjust', 'affirmative', 'affirmatory', 'all right', 'alright', 'approbative', 'approbatory', 'approve', 'approving', 'aright', 'by all odds', 'castigate', 'certain', 'certainly', 'chasten', 'chastise', 'compensate', 'correct', 'correctly', 'counterbalance', 'dead', 'decent', 'decently', 'decidedly', 'decline', 'definitely', 'discipline', 'emphatically', 'even off', 'even out', 'even up', 'exactly', 'fine', 'flop', 'for certain', 'for sure', 'good', 'hunky-dory', 'in good order', 'in spades', 'incisively', 'indeed', 'indisputable', 'just', 'justly', 'make up', 'mightily', 'mighty', 'o.k.', 'objurgate', 'ok', 'okay', 'okeh', 'okey', 'on the button', 'on the dot', 'on the nose', 'optimistic', 'perfectly', 'plausive', 'powerful', 'precisely', 'proper', 'properly', 'rectify', 'redress', 'right', 'right field', 'right hand', 'right on', 'right wing', 'right-hand', 'rightfield', 'rightfulness', 'ripe', 'sanction', 'set', 'slump', 'so', 'sort out', 'sure', 'sure as shooting', 'sure enough', 'surely', 'the right way', 'trusted', 'unquestionably', 'utterly', 'veracious', 'very well', 'yea', 'yeah', 'yep', 'yes']\n"
     ]
    }
   ],
   "source": [
    "# generate confirmation words set from seed words\n",
    "from nltk.corpus import wordnet as wn\n",
    "seed = {'yes', 'yeah', 'yep', 'sure', 'okay', 'ok', 'indeed', 'exactly', 'right',\n",
    "        'correct', 'absolutely', 'definitely', 'certainly', 'affirmative'}\n",
    "confirmation_words = set(seed)\n",
    "for word in list(seed):\n",
    "    for syn in wn.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            confirmation_words.add(lemma.name().replace('_', ' '))\n",
    "\n",
    "print(sorted(confirmation_words))\n",
    "confirmation_words = list(confirmation_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4cdbc545",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  negation word list\n",
    "negation_words = {\n",
    "    \"no\", \"not\", \"n't\", \"never\", \"none\", \"nothing\", \"nobody\", \"nowhere\",\n",
    "    \"neither\", \"nor\", \"barely\", \"hardly\", \"scarcely\", \"seldom\", \"little\",\n",
    "    \"few\", \"without\", \"lack\", \"lacking\", \"cannot\", \"can't\", \"won't\",\n",
    "    \"wouldn't\", \"shouldn't\", \"couldn't\", \"didn't\", \"doesn't\", \"don't\",\n",
    "    \"isn't\", \"aren't\", \"wasn't\", \"weren't\", \"haven't\", \"hasn't\", \"hadn't\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f13154ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chandler:\n",
      "          25,624 tokens,\n",
      "          23,366 vocabulary,\n",
      "          2,258 total repetitions,\n",
      "          1,318 confirmation words,\n",
      "          853 negation words.\n",
      "Joey:\n",
      "          26,148 tokens,\n",
      "          24,023 vocabulary,\n",
      "          2,125 total repetitions,\n",
      "          1,278 confirmation words,\n",
      "          862 negation words.\n",
      "Rachel:\n",
      "          27,155 tokens,\n",
      "          24,625 vocabulary,\n",
      "          2,530 total repetitions,\n",
      "          1,551 confirmation words,\n",
      "          900 negation words.\n",
      "Monica:\n",
      "          23,215 tokens,\n",
      "          21,479 vocabulary,\n",
      "          1,736 total repetitions,\n",
      "          960 confirmation words,\n",
      "          682 negation words.\n",
      "Phoebe:\n",
      "          22,077 tokens,\n",
      "          20,172 vocabulary,\n",
      "          1,905 total repetitions,\n",
      "          1,372 confirmation words,\n",
      "          751 negation words.\n",
      "Ross:\n",
      "          25,296 tokens,\n",
      "          22,579 vocabulary,\n",
      "          2,717 total repetitions,\n",
      "          1,338 confirmation words,\n",
      "          854 negation words.\n"
     ]
    }
   ],
   "source": [
    "# get the vocabulary set and size associated to each speaker.\n",
    "from lib2to3.pgen2 import token\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r\"[A-Za-z']+\")\n",
    "\n",
    "def build_speaker_vocab_info(df, speaker_name):\n",
    "    utterancesArr  = df['utterance'].str.lower().tolist()\n",
    "    totalWordRep = 0\n",
    "    totalVocab = []\n",
    "    totalTokens = []\n",
    "    negation_tokens_count = 0\n",
    "    confirmation_words_in_utterances = []\n",
    "    for sentence in utterancesArr:\n",
    "        tokens = tokenizer.tokenize(sentence)\n",
    "        for token in tokens:\n",
    "            if token in confirmation_words:                \n",
    "                confirmation_words_in_utterances.append(token)\n",
    "            if token in negation_words:\n",
    "                negation_tokens_count += 1\n",
    "        vocab = sorted(set(tokens))\n",
    "        totalVocab.extend(vocab)\n",
    "        totalTokens.extend(tokens)\n",
    "        dup = len(tokens) - len(vocab)\n",
    "        totalWordRep += dup\n",
    "    return {'tokens': totalTokens, 'vocab': totalVocab, \n",
    "            'totalWordRep': totalWordRep, \n",
    "            'confirmation_words':confirmation_words_in_utterances,\n",
    "            'negation_tokens_count': negation_tokens_count}\n",
    "\n",
    "utterance_features = {}\n",
    "for speaker_name in distinct_speakers1:\n",
    "    current_speaker_utterance_df = globals()[f'df_{speaker_name}']\n",
    "    utterance_features[speaker_name] = build_speaker_vocab_info(current_speaker_utterance_df, speaker_name)\n",
    "    print(f\"\"\"{speaker_name}:\n",
    "          {len(utterance_features[speaker_name][\"tokens\"]):,} tokens,\n",
    "          {len(utterance_features[speaker_name][\"vocab\"]):,} vocabulary,\n",
    "          {utterance_features[speaker_name][\"totalWordRep\"]:,} total repetitions,\n",
    "          {len(utterance_features[speaker_name][\"confirmation_words\"]):,} confirmation words,\n",
    "          {utterance_features[speaker_name][\"negation_tokens_count\"]:,} negation words.\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
